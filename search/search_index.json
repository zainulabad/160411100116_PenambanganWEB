{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"crawler-structure-web/","text":"CRAWLER STRUCTURE WEBSITE \u00b6 ================================================== Nama : Zainul Abad NIM : 160411100116 Mata Kuliah : Penambangan WEB. ================================================== Target Link Website : https://autonetmagz.com Program yang dibutuhkan : Python 3.6 (Jupyter Notebook) library : beautifulsoup4 requests networkx matplotlib import library requests, bs4 , networkx dan matplotlib. import tersebut digunakan untuk mngambil data dari sebuah website dan membuat graph. # untuk crawling link import requests from bs4 import BeautifulSoup # untuk membuat graph import networkx as nx import matplotlib.pyplot as plt Selanjutnya kita menggunakan code dibawah untuk menyamakan format url. agar tidak ada url yang sama. def simplifiedURL ( url ): if \"www.\" in url : #untuk mengecek www ind = url . index ( \"www.\" ) + 4 url = url [ ind :] if not \"http\" in url : #untuk mengecek http/https url = \"http://\" + url if url [ - 1 ] == \"/\" : #untuk garing penutup link url = url [: - 1 ] return url selanjutnya kita menggunakan code dibawah untuk mendapatkan atau mengambil semua link dalam website yang akan di crawl . def getAllLinks ( src ): #mencegah error apabila link yang terdapat di html nya tidak berfungsi try : #mengambil halaman html ind = src . find ( ':' ) + 3 url = src [ ind :] page = requests . get ( src ) #mengubah html ke object dengan beautiful soup soup = BeautifulSoup ( page . content , 'html.parser' ) #mengambil semua tag <a> yang berada di html karena link di simpan di dalam tag <a> tags = soup . findAll ( \"a\" ) links = [] for tag in tags : #mencegah error apabila link tidak memiliki href try : link = tag [ 'href' ] if not link in links and 'http' in link : #memasukkan link yang didapat didalam list links . append ( link ) except KeyError : pass return links except : return list () kemudian kita melakukan crawling berdasarkan 4 parameter yaitu (url, max_deep, show=False, deep=0) def crawl ( url , max_deep , show = False , deep = 0 ): # returnnya ada di edgelist, global edgelist # menyamakan format url, agar tidak ada url yg dobel url = simplifiedURL ( url ) # crawl semua link links = getAllLinks ( url ) # menambah counter kedalaman deep += 1 #menampilkan proses if show : if deep == 1 : print ( \"( %d ) %s \" % ( len ( links ), url )) else : print ( \"|\" , end = \"\" ) for i in range ( deep - 1 ): print ( \"--\" , end = \"\" ) print ( \"( %d ) %s \" % ( len ( links ), url )) for link in links : # Membentuk format jalan (edge => (dari, ke)) edge = ( url , link ) # Mengecek jalan, apabila belum dicatat, maka dimasukkan ke list if not edge in edgelist : edgelist . append ( edge ) # Cek kedalaman, jika belum sampai terakhir, maka crawling. if ( deep != max_deep ): crawl ( link , max_deep , show , deep ) - url : untuk alamat website yang akan di crawling - max_deep : untuk maksimal kedalaman crawling - show : untuk menampilkan proses crawling, dan saya berikan kondisi boolean false agar tidak ditampilkan. - deep : untuk mengecek kedalaman proses crawling. # Inisialisasi variabel awal root = \"https://autonetmagz.com/\" nodelist = [ root ] done = [ root ] edgelist = [] #Proses crawl tampilkan = True crawl ( root , 3 , show = tampilkan ) if tampilkan : for i in range ( 10 ): print ( \"\" ) Selanjutnya kita membentuk graph dari link yang sudah kita dapatkan tadi. code berikut yang digunakan. #membuat Graph g = nx . Graph () #mengubah graph menjadi graph berarah g = g . to_directed () # Masukin ke Graph g . add_edges_from ( edgelist ) # deklarasi pos (koordinat) (otomatis) pos = nx . spring_layout ( g ) # hitung pagerank pr = nx . pagerank ( g ) # Membuat Label && print pagerank print ( \"keterangan node:\" ) nodelist = g . nodes label = {} for i , key in enumerate ( nodelist ): label [ key ] = i print ( i , key , pr [ key ]) # Draw Graph #plt.figure(1) plt . title ( 'circle_layout' ) nx . draw ( g , pos ) nx . draw_networkx_labels ( g , pos , label ) # show figure plt . axis ( \"off\" ) plt . show () hasil dari Crawl dan tampilan graph : Referensi :","title":"Crawling Website Structure"},{"location":"crawler-structure-web/#crawler-structure-website","text":"================================================== Nama : Zainul Abad NIM : 160411100116 Mata Kuliah : Penambangan WEB. ================================================== Target Link Website : https://autonetmagz.com Program yang dibutuhkan : Python 3.6 (Jupyter Notebook) library : beautifulsoup4 requests networkx matplotlib import library requests, bs4 , networkx dan matplotlib. import tersebut digunakan untuk mngambil data dari sebuah website dan membuat graph. # untuk crawling link import requests from bs4 import BeautifulSoup # untuk membuat graph import networkx as nx import matplotlib.pyplot as plt Selanjutnya kita menggunakan code dibawah untuk menyamakan format url. agar tidak ada url yang sama. def simplifiedURL ( url ): if \"www.\" in url : #untuk mengecek www ind = url . index ( \"www.\" ) + 4 url = url [ ind :] if not \"http\" in url : #untuk mengecek http/https url = \"http://\" + url if url [ - 1 ] == \"/\" : #untuk garing penutup link url = url [: - 1 ] return url selanjutnya kita menggunakan code dibawah untuk mendapatkan atau mengambil semua link dalam website yang akan di crawl . def getAllLinks ( src ): #mencegah error apabila link yang terdapat di html nya tidak berfungsi try : #mengambil halaman html ind = src . find ( ':' ) + 3 url = src [ ind :] page = requests . get ( src ) #mengubah html ke object dengan beautiful soup soup = BeautifulSoup ( page . content , 'html.parser' ) #mengambil semua tag <a> yang berada di html karena link di simpan di dalam tag <a> tags = soup . findAll ( \"a\" ) links = [] for tag in tags : #mencegah error apabila link tidak memiliki href try : link = tag [ 'href' ] if not link in links and 'http' in link : #memasukkan link yang didapat didalam list links . append ( link ) except KeyError : pass return links except : return list () kemudian kita melakukan crawling berdasarkan 4 parameter yaitu (url, max_deep, show=False, deep=0) def crawl ( url , max_deep , show = False , deep = 0 ): # returnnya ada di edgelist, global edgelist # menyamakan format url, agar tidak ada url yg dobel url = simplifiedURL ( url ) # crawl semua link links = getAllLinks ( url ) # menambah counter kedalaman deep += 1 #menampilkan proses if show : if deep == 1 : print ( \"( %d ) %s \" % ( len ( links ), url )) else : print ( \"|\" , end = \"\" ) for i in range ( deep - 1 ): print ( \"--\" , end = \"\" ) print ( \"( %d ) %s \" % ( len ( links ), url )) for link in links : # Membentuk format jalan (edge => (dari, ke)) edge = ( url , link ) # Mengecek jalan, apabila belum dicatat, maka dimasukkan ke list if not edge in edgelist : edgelist . append ( edge ) # Cek kedalaman, jika belum sampai terakhir, maka crawling. if ( deep != max_deep ): crawl ( link , max_deep , show , deep ) - url : untuk alamat website yang akan di crawling - max_deep : untuk maksimal kedalaman crawling - show : untuk menampilkan proses crawling, dan saya berikan kondisi boolean false agar tidak ditampilkan. - deep : untuk mengecek kedalaman proses crawling. # Inisialisasi variabel awal root = \"https://autonetmagz.com/\" nodelist = [ root ] done = [ root ] edgelist = [] #Proses crawl tampilkan = True crawl ( root , 3 , show = tampilkan ) if tampilkan : for i in range ( 10 ): print ( \"\" ) Selanjutnya kita membentuk graph dari link yang sudah kita dapatkan tadi. code berikut yang digunakan. #membuat Graph g = nx . Graph () #mengubah graph menjadi graph berarah g = g . to_directed () # Masukin ke Graph g . add_edges_from ( edgelist ) # deklarasi pos (koordinat) (otomatis) pos = nx . spring_layout ( g ) # hitung pagerank pr = nx . pagerank ( g ) # Membuat Label && print pagerank print ( \"keterangan node:\" ) nodelist = g . nodes label = {} for i , key in enumerate ( nodelist ): label [ key ] = i print ( i , key , pr [ key ]) # Draw Graph #plt.figure(1) plt . title ( 'circle_layout' ) nx . draw ( g , pos ) nx . draw_networkx_labels ( g , pos , label ) # show figure plt . axis ( \"off\" ) plt . show () hasil dari Crawl dan tampilan graph : Referensi :","title":"CRAWLER STRUCTURE WEBSITE"},{"location":"webcrawler/","text":"WEB CRAWLER \u00b6 ================================================== Nama : Zainul Abad NIM : 160411100116 Mata Kuliah : Penambangan WEB. ================================================== APA ITU WEB CRAWLER ? \u00b6 Web Crawler adalah sebuah program dengan metode tertentu yang berfungsi untuk melakukan Scan atau crawl ke semua halaman internet untuk mencari data yang diinginkan. Pada WEB Crawl kita membutuhkan Link Web. pada web crawl kali ini saya menggunakan Jupyter (Python versi 3) , dan pada Python ini saya menggunakan library : BeautifulSoup4 Requesests csv SQLite numpy scipy scikit-learn scikit-fuzzy => Jadi pada Tahap Pertama Kita mencari link web yang akan kita crawl. Link : https://www.suara.com/bola/bola-category/bola-indonesia kita request link: req = Request ( \"https://www.suara.com/bola/bola-category/bola-indonesia\" ) html_page = urlopen ( req ) soup = BeautifulSoup ( html_page , \"lxml\" ) ` => Langkah selanjutnya kita mencari tahu tag pada artikel tersebut, kita perlu inspect elemen. tag pada artikel saya . Tag judul \"h4\" dengan class=\"past-title\" source code dari program saya : html = urlopen ( \"https://www.suara.com/bola/bola-category/bola-indonesia\" ) . read () soup = BeautifulSoup ( html , \"lxml\" ) ar = soup . find_all ( \"li\" , \"item-outer\" ) i = 1 judul = [] for j in ar : dapat_judul = j . find ( 'h4' , 'post-title' ) . get_text () . replace ( \" \\n \" , \"\" ) judul . append ( dapat_judul ) print ( judul ) => Code diatas digunakan untuk mendapatkan judul dan deskripsi pada artikel yang kita crawling. ========================== => kemudian kita gunakan code dibawah untuk mengambil link. dan linknya berada dalam tag 'a' dengan class 'ellipsis2' links = [] deskripsifull = [] for link in soup . findAll ( 'a' , 'ellipsis2' ): links . append ( link . get ( 'href' )) #menyimpan link print ( links ) ========================== => Source Code dibawah ini digunakan untuk jika ada kesalahan saat crawl. jika ada data yang sama data tersebut tidak dimasukkan di array. ` des = [] for i in links : deslink = urlopen ( i ) . read () #membuka link 1per1 soup1 = BeautifulSoup ( deslink , \"lxml\" ) ketdes = soup1 . find_all ( \"article\" ) da = [] for j in ketdes : desk = \"\" konten = soup1 . find_all ( 'article' , 'content-article' ) for i in soup1 . find ( 'article' , 'content-article' ) . find_all ( 'p' ): desk = desk + i . text if ( not desk in des ): des . append ( desk ) print ( len ( des )) ` ========================== => Source code dibawah ini digunakan untuk membuang karakter dan space yang tidak dibutuhkan. for j in des : print ( j . replace ( '\"' , ' ' ) . replace ( '.' , ' ' ) . replace ( '/' , ' ' ) . replace ( ',' , ' ' ) . replace ( '\"\"' , ' ' )) print ( \"==========================================\" ) ========================== import pandas as pd ` `import numpy as np` `artikel = {'judul':judul,'deskripsi':des}` `df=pd.DataFrame(artikel,columns=['judul','deskripsi'])` `df.to_csv(\"dataku.csv\",sep=',')` `df.sort_values('judul',ascending=True)` `per_kata=[]` `for kt in des:` `per_kata.append(kt.split())` `print(per_kata)` ` print ( len ( des )) ========== import pandas as pd data = pd . read_csv ( \"dataku.csv\" ) deskr = [] for i in data [ 'deskripsi' ]: deskr . append ( i ) print ( deskr ) => Source Code diatas digunakan untuk import judul dan deskripsi ke data CSV. ========================== TEXT PROCESSING \u00b6 Tokenisasi - Tokenisasi adalah proses untuk membagi teks yang dapat berupa kalimat, paragraf atau dokumen, menjadi token-token. Sebagai contoh, tokenisasi dari kalimat \" Pelajar Indonesia di China Ditemukan Tewas. \" menghasilkan enam token, yakni: \" Pelajar \", \" Indonesia \", \" di \", \" China \", \" Ditemukan \", \" Tewas \". Biasanya, yang menjadi acuan pemisah antar token adalah spasi dan tanda baca. Tokenisasi berguna untuk analisis teks lebih lanjut dan dipakai dalam ilmu linguistik. Filtering - Tahap Filtering adalah tahap mengambil kata-kata penting dari hasil token. Bisa menggunakan algoritma stoplist (membuang kata kurang penting) atau wordlist (menyimpan kata penting). Stoplist/stopword adalah kata-kata yang tidak deskriptif yang dapat dibuang dalam pendekatan bag-of-words. Contoh stopwords adalah \u201cyang\u201d, \u201cdan\u201d, \u201cdi\u201d, \u201cdari\u201d dan seterusnya Stemming - Stemming adalah proses pemetaan dan penguraian bentuk dari suatu kata menjadi bentuk kata dasarnya. Gampangnya, proses mengubah kata berimbuhan menjadi kata dasar Stemming (atau mungkin lebih tepatnya lemmatization?) adalah proses mengubah kata berimbuhan menjadi kata dasar. Aturan-aturan bahasa diterapkan untuk menanggalkan imbuhan-imbuhan itu. Contohnya : Bermain => Main Berolahraga => Olahraga ========================== => Source Code library dibawah ini digunakan untuk import csv from sastrawi. library ini digunakan untuk mengambil kata dasar pada artikel yang kita crawling. `` import csv from Sastrawi.Stemmer.StemmerFactory import StemmerFactory from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory SWfactory = StopWordRemoverFactory () factory = StemmerFactory () stemmer = factory . create_stemmer () stopword = SWfactory . create_stop_word_remover () Sfactory = StemmerFactory () ========================== => Source Code dibawah ini digunakan untuk memisah kata dasar pada sebuah artikel yang kita crawling. `` f = open ( 'dataku.csv' , 'r' ) reader = csv . reader ( f ) deskripsi = []; judul = []; jumlah = 0 for row in reader : jumlah += 1 ; if jumlah != 1 : judul . append ( str ( row [ 1 ])) deskripsi . append ( str ( row [ 2 ])) des_katadasar = [] for i in deskripsi : hasil = '' ` kata_per_artikel = {} for j in i . split (): if j . isalpha (): stop = stopword . remove ( j ) stem = stemmer . stem ( stop ) hasil += stem + ' ' des_katadasar . append ( hasil ) setelah melalui tahap ini kita akan membuat VSM atau Vector Space Model. ========================== VECTOR SPACE MODEL (VSM) \u00b6 Pada VSM sebuah dokumen akan direpresentasikan dalam sebuah vektor, dimana setiap nilai pada vektor tersebut mewakili bobot term yang bersangkutan. adalah model aljabar yang merepresentasikan kumpulan dokumen sebagai vetctor. VSM dapat diaplikasikan dalam klasifikasi dokumen, clustering dokumen, dan scoring dokumen terhadap sebuah query. Dalam VSM setiap dokumen direpresentasikan sebagai sebuah vector, dimana nilai dari setiap nilai dari vector tersebut mewakili weight sebuah term. Dalam menghitung term-weight dapa digunakan beberapa teknik seperti frequency, binary-document vector, atau tf-idf. Contoh : Artikel 1 : siswa itu sedang belajar Artikel 2 : siswa itu sedang bermain bola kita akan menghitung setiap kata dan kita tampung pada table CSV. \u200b Siswa itu sedang belajar bermain bola Artikal1 1 1 1 0 0 0 Artikel2 1 1 1 0 1 1 Untuk Codenya : => Source Code dibawah digunakan untuk menghitung setiap kata. def countWord(txt, ngram=1): d = dict() token = tokenisasi(txt, ngram) for i in token: if d.get(i) == None: d[i] = txt.count(i) return d => Source Code dibawah digunakan untuk membangun VSM. `def add_row_VSM(d): VSM.append([]) for i in VSM[0]: if d.get(i) == None: VSM[-1].append(0) else : VSM[-1].append(d.pop(i)); for i in d: VSM[0].append(i) for j in range(1, len(VSM)-1): VSM[j].insert(-2,0) \u00b6 VSM[j].append(0) VSM[-1].append(d.get(i))` `cursor = conn.execute(\"SELECT * from jurnal2\") cursor = cursor.fetchall() cursor = cursor[:60] pertama = True corpus = list() label = list() c=1 n = int(input(\"ngram : \")) n=1 \u00b6 for row in cursor: print ('Proses : %.2f' %((c/len(cursor))*100) + '%'); c+=1 label.append(row[-1]) txt = row[-2] cleaned = preprosesing(txt) cleaned = cleaned[:-1] corpus.append(cleaned) d = countWord(cleaned, n) if pertama: pertama = False VSM = list((list(), list())) for key in d: VSM[0].append(key) VSM[1].append(d[key]) else: add_row_VSM(d)` => Source Code dibawah untuk menampilkan hasil CSV yang sudah kita create tadi. write_csv(\"bow_manual_%d.csv\"%n, VSM) feature_name = VSM[0] bow = np.array(VSM[1:]) ========================== TF - IDF \u00b6 TF - TF atau term frequency adalah weighting scheme yang digunakan untuk menentukan relevansi dokumen dengan sebuah query (term). TF menentukan bobot relevansi sebuah dokumen dan term berdasarkan frekuensi kemunculan term pada dokumen terkait. Untuk menghitung TF terdapat beberapa jenis fungsi yang dapat digunakan DF - Jika TF adalah frekuensi kemunculan suatu term pada sebuah dokumen, DF merupakan jumlah dokumen dimana terdapat term yang bersangkutan. Konsep DF sendiri dilatarbelakangin oleh masalah pada TF, dimana semua term dianggap sama penting, sehingga term yang memiliki sedikit atau tidak memiliki discrimination power dapat mempengaruhi akurasi dalam menentukan relevansi antara term dan dokumen. Ide dari DF adalah dengan mengurangi bobot TF suatu term dengan membaginya dengan frekuensi term terhadap koleksi dokumen (DF). Jadi sebuah term yang memiliki bobot TF yang besar namun dengan bobot DF yang besar pula tidak akan memiliki pengaruh yang besar dalam menentukan sebuah relevansi . IDF - IDF adalah inverse dari DF, IDF akan melakukan proses scaling pada TF. Term yang memiliki DF yang rendah akan memiliki IDF yang tinggi. Dengan kata lain, sebuah term yang jarang ditemui pada koleksi dokumen atau bisa dikatakan sebagai term khusus akan memiliki nilai IDF yang tinggi. Untuk menghitung IDF pada sebuah term pada sebuah koleksi dokumen dapat menggunakan fungsi dibawah ini, TF - IDF -- Selain menggunakan Bag of Words, kita juga bisa menggunakan metode TF-IDF. Hal ini karena Bag of Word memiliki kelemahan tersendiri. TF-IDF sendiri merupakan kepanjangan dari Term Frequence (frekuensi Kata) dan Invers Document Frequence (invers frekuensi Dokumen). Rumus TF-IDF sendiri terbilang mudah karena hanya TFxIDF. Kita telah mencari TF sebelumnya (yaitu Bag of Words), karena konsep keduanya yang memang sama. Sekarang kita tinggal mencari nilai IDF. Untuk mendapatkan IDF, pertama kita perlu mencari DF (frekuensi Dokumen). Contoh : Artikel 1 : siswa itu sedang belajar Artikel 2 : siswa itu sedang bermain bola Kata Jumlah Kata siswa 2 itu 2 sedang 2 belajar 1 bermain 1 bola 1 => Source Code df = list() total_doc = bow.shape[0] for kolom in range(len(bow[0])): total = 0 for baris in range(len(bow)): if (bow[baris, kolom] > 0): total +=1 df.append(total) df = np.array(df) idf = list() for i in df: tmp = 1 + log10(total_doc/(1+i)) idf.append(tmp) idf = np.array(idf) tfidf = bow * idf ========================== PREPROCESSING \u00b6 Tahap preprocessing merupakan tahap mengolah data agar data lebih mudah diproses. Untuk teman-teman tidak diharuskan mengunakaan python saja, namun terserah teman-teman Tpi pada penejalasan ini saya menggunakan python Seleksi Fitur - Seleksi fitur merupakan salah satu cara untuk mengurangi dimensi fitur yang sangat banyak. Seperti pada kasus kita, Text Mining, jumlah fitur yang didapatkan bisa mencapai lebih dari 2000 kata yang berbeda. Namun, tidak semua kata tersebut benar-benar berpengaruh pada hasil akhir nantinya. Selain itu, kita tahu bahwa semakin banyak data yang diproses, maka lebih banyak biaya dan waktu yang digunakan untuk memprosesnya. Oleh karena itu, kita perlu melakukan pengurangan fitur tanpa mengurangi kualitas hasil akhir, misalnya dengan Seleksi Fitur. Pada dasarnya, seleksi fitur memiliki 3 tipe umum: Wrap Filter Embed => Source Code Seleksi Fitur def seleksiFiturPearson(data, threshold): global meanFitur meanFitur = meanF(data) u=0 while u < len(data[0]): dataBaru=data[:, :u+1] meanBaru=meanFitur[:u+1] v = u while v < len(data[0]): if u != v: value = pearsonCalculate(data, u,v) if value < threshold: dataBaru = np.hstack((dataBaru, data[:, v].reshape(data.shape[0],1))) meanBaru = np.hstack((meanBaru, meanFitur[v])) v+=1 data = dataBaru meanFitur=meanBaru if u%50 == 0 : print(u, data.shape) u+=1 return data ========================== Pearson Corrlation Pendekatan Pearson merupakan pendekatan paling sederhana. Pada pendekatan ini, setiap fitur akan dihitung korelasinya. Semakin tinggi nilainya, maka fitur tersebut semakin kuat korelasinya. Lalu fitur yang memiliki korelasi tinggi akan dibuang salah satunya. Pendekatan ini digunakan untuk data tipe numerik. Kodenya bisa dilihat dibawah ini def pearsonCalculate(data, u,v): \"i, j is an index\" atas=0; bawah_kiri=0; bawah_kanan = 0 for k in range(len(data)): atas += (data[k,u] - meanFitur[u]) * (data[k,v] - meanFitur[v]) bawah_kiri += (data[k,u] - meanFitur[u])**2 bawah_kanan += (data[k,v] - meanFitur[v])**2 bawah_kiri = bawah_kiri ** 0.5 bawah_kanan = bawah_kanan ** 0.5 return atas/(bawah_kiri * bawah_kanan) def meanF(data): meanFitur=[] for i in range(len(data[0])): meanFitur.append(sum(data[:,i])/len(data)) return np.array(meanFitur) ========================== Clustering Clustering merupakan pengelompokan data menjadi k-kelompok (dengan k merupakan banyak kelompok). Pengelompkan tersebut berdasarkan ciri yang mirip. Pada kasus ini, maka ciri yang mirip bisa diketahui dari kata yang menjadi ciri dari setiap dokumen. Metode Clustering sendiri ada banyak. Salah duanya adalah K-Means Clustering dan Fuzzy C-Means Clustering. Setelah dilakukan proses Clustering, perlu kita cari nilai Silhouette Coefficient untuk melihat apakah hasil cluster tersebut sudah bagus atau tidak. ========================== K-Means => Source Code dibawah ini digunakan untuk melakukan clustering. kmeans = KMeans(n_clusters=5, random_state=0).fit(tfidf_matrix.todense()) for i in range(len(kmeans.labels_)): print(\"Doc %d =>> cluster %d\" %(i+1, kmeans.labels_[i])) ========================== Klasifikasi Berbeda dengan Clustering yang mengelompokkan data, klasifikasi lebih mirip pada peramalan data dengan merujuk kategori. Contoh simplenya, seperti peramalan golf. Jika cuaca cerah, angin tidak kencang, maka akan diadakan pertandingan golf. Jika cuaca cerah tapi angin kencang, maka pertandingan golf dibatalkan. Proses ini biasanya memerlukan data latih dan data test. Untuk menguji keberhasilannya, diperlukan untuk menghitung akurasi atau confution matrix. Metode-metode yang sering dipakai untuk klasifikasi di antaranya Naive Bayes. ========================== Naive Bayes Naive bayes termasuk pada supervised learning berdasarkan teorema Bayes, dengan asumsi \"naive\", bahwa setiap fitur tidak saling terkait (independen). Terdapat beberapa jenis Naive Bayes, seperti Gaussian NB, Multinomial NB, dll. Namun pada kasus text classification, Multinomial lebih cocok digunakan. Sebelum itu, kita perlu membagi data menjadi dua bagian: data training, dan data test x_train, x_test, y_train, y_test = train_test_split(xBaru1, label, test_size=0.33, random_state=0) model = MultinomialNB().fit(x_train, y_train) predicted = model.predict(x_test) ========================== REFERENSI \u00b6 https://github.com/sastrawi/sastrawi/wiki/Stemming-Bahasa-Indonesia https://www.codepolitan.com/stemming-word-dalam-carik-bot-59a9ef6e96088 https://www.suara.com/bola/bola-category/bola-indonesia http://id.dbpedia.org/page/Tokenisasi https://github.com https://python.org https://jupyter.org","title":"Web Crawler"},{"location":"webcrawler/#web-crawler","text":"================================================== Nama : Zainul Abad NIM : 160411100116 Mata Kuliah : Penambangan WEB. ==================================================","title":"WEB CRAWLER"},{"location":"webcrawler/#apa-itu-web-crawler","text":"Web Crawler adalah sebuah program dengan metode tertentu yang berfungsi untuk melakukan Scan atau crawl ke semua halaman internet untuk mencari data yang diinginkan. Pada WEB Crawl kita membutuhkan Link Web. pada web crawl kali ini saya menggunakan Jupyter (Python versi 3) , dan pada Python ini saya menggunakan library : BeautifulSoup4 Requesests csv SQLite numpy scipy scikit-learn scikit-fuzzy => Jadi pada Tahap Pertama Kita mencari link web yang akan kita crawl. Link : https://www.suara.com/bola/bola-category/bola-indonesia kita request link: req = Request ( \"https://www.suara.com/bola/bola-category/bola-indonesia\" ) html_page = urlopen ( req ) soup = BeautifulSoup ( html_page , \"lxml\" ) ` => Langkah selanjutnya kita mencari tahu tag pada artikel tersebut, kita perlu inspect elemen. tag pada artikel saya . Tag judul \"h4\" dengan class=\"past-title\" source code dari program saya : html = urlopen ( \"https://www.suara.com/bola/bola-category/bola-indonesia\" ) . read () soup = BeautifulSoup ( html , \"lxml\" ) ar = soup . find_all ( \"li\" , \"item-outer\" ) i = 1 judul = [] for j in ar : dapat_judul = j . find ( 'h4' , 'post-title' ) . get_text () . replace ( \" \\n \" , \"\" ) judul . append ( dapat_judul ) print ( judul ) => Code diatas digunakan untuk mendapatkan judul dan deskripsi pada artikel yang kita crawling. ========================== => kemudian kita gunakan code dibawah untuk mengambil link. dan linknya berada dalam tag 'a' dengan class 'ellipsis2' links = [] deskripsifull = [] for link in soup . findAll ( 'a' , 'ellipsis2' ): links . append ( link . get ( 'href' )) #menyimpan link print ( links ) ========================== => Source Code dibawah ini digunakan untuk jika ada kesalahan saat crawl. jika ada data yang sama data tersebut tidak dimasukkan di array. ` des = [] for i in links : deslink = urlopen ( i ) . read () #membuka link 1per1 soup1 = BeautifulSoup ( deslink , \"lxml\" ) ketdes = soup1 . find_all ( \"article\" ) da = [] for j in ketdes : desk = \"\" konten = soup1 . find_all ( 'article' , 'content-article' ) for i in soup1 . find ( 'article' , 'content-article' ) . find_all ( 'p' ): desk = desk + i . text if ( not desk in des ): des . append ( desk ) print ( len ( des )) ` ========================== => Source code dibawah ini digunakan untuk membuang karakter dan space yang tidak dibutuhkan. for j in des : print ( j . replace ( '\"' , ' ' ) . replace ( '.' , ' ' ) . replace ( '/' , ' ' ) . replace ( ',' , ' ' ) . replace ( '\"\"' , ' ' )) print ( \"==========================================\" ) ========================== import pandas as pd ` `import numpy as np` `artikel = {'judul':judul,'deskripsi':des}` `df=pd.DataFrame(artikel,columns=['judul','deskripsi'])` `df.to_csv(\"dataku.csv\",sep=',')` `df.sort_values('judul',ascending=True)` `per_kata=[]` `for kt in des:` `per_kata.append(kt.split())` `print(per_kata)` ` print ( len ( des )) ========== import pandas as pd data = pd . read_csv ( \"dataku.csv\" ) deskr = [] for i in data [ 'deskripsi' ]: deskr . append ( i ) print ( deskr ) => Source Code diatas digunakan untuk import judul dan deskripsi ke data CSV. ==========================","title":"APA ITU WEB CRAWLER ?"},{"location":"webcrawler/#text-processing","text":"Tokenisasi - Tokenisasi adalah proses untuk membagi teks yang dapat berupa kalimat, paragraf atau dokumen, menjadi token-token. Sebagai contoh, tokenisasi dari kalimat \" Pelajar Indonesia di China Ditemukan Tewas. \" menghasilkan enam token, yakni: \" Pelajar \", \" Indonesia \", \" di \", \" China \", \" Ditemukan \", \" Tewas \". Biasanya, yang menjadi acuan pemisah antar token adalah spasi dan tanda baca. Tokenisasi berguna untuk analisis teks lebih lanjut dan dipakai dalam ilmu linguistik. Filtering - Tahap Filtering adalah tahap mengambil kata-kata penting dari hasil token. Bisa menggunakan algoritma stoplist (membuang kata kurang penting) atau wordlist (menyimpan kata penting). Stoplist/stopword adalah kata-kata yang tidak deskriptif yang dapat dibuang dalam pendekatan bag-of-words. Contoh stopwords adalah \u201cyang\u201d, \u201cdan\u201d, \u201cdi\u201d, \u201cdari\u201d dan seterusnya Stemming - Stemming adalah proses pemetaan dan penguraian bentuk dari suatu kata menjadi bentuk kata dasarnya. Gampangnya, proses mengubah kata berimbuhan menjadi kata dasar Stemming (atau mungkin lebih tepatnya lemmatization?) adalah proses mengubah kata berimbuhan menjadi kata dasar. Aturan-aturan bahasa diterapkan untuk menanggalkan imbuhan-imbuhan itu. Contohnya : Bermain => Main Berolahraga => Olahraga ========================== => Source Code library dibawah ini digunakan untuk import csv from sastrawi. library ini digunakan untuk mengambil kata dasar pada artikel yang kita crawling. `` import csv from Sastrawi.Stemmer.StemmerFactory import StemmerFactory from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory SWfactory = StopWordRemoverFactory () factory = StemmerFactory () stemmer = factory . create_stemmer () stopword = SWfactory . create_stop_word_remover () Sfactory = StemmerFactory () ========================== => Source Code dibawah ini digunakan untuk memisah kata dasar pada sebuah artikel yang kita crawling. `` f = open ( 'dataku.csv' , 'r' ) reader = csv . reader ( f ) deskripsi = []; judul = []; jumlah = 0 for row in reader : jumlah += 1 ; if jumlah != 1 : judul . append ( str ( row [ 1 ])) deskripsi . append ( str ( row [ 2 ])) des_katadasar = [] for i in deskripsi : hasil = '' ` kata_per_artikel = {} for j in i . split (): if j . isalpha (): stop = stopword . remove ( j ) stem = stemmer . stem ( stop ) hasil += stem + ' ' des_katadasar . append ( hasil ) setelah melalui tahap ini kita akan membuat VSM atau Vector Space Model. ==========================","title":"TEXT PROCESSING"},{"location":"webcrawler/#vector-space-model-vsm","text":"Pada VSM sebuah dokumen akan direpresentasikan dalam sebuah vektor, dimana setiap nilai pada vektor tersebut mewakili bobot term yang bersangkutan. adalah model aljabar yang merepresentasikan kumpulan dokumen sebagai vetctor. VSM dapat diaplikasikan dalam klasifikasi dokumen, clustering dokumen, dan scoring dokumen terhadap sebuah query. Dalam VSM setiap dokumen direpresentasikan sebagai sebuah vector, dimana nilai dari setiap nilai dari vector tersebut mewakili weight sebuah term. Dalam menghitung term-weight dapa digunakan beberapa teknik seperti frequency, binary-document vector, atau tf-idf. Contoh : Artikel 1 : siswa itu sedang belajar Artikel 2 : siswa itu sedang bermain bola kita akan menghitung setiap kata dan kita tampung pada table CSV. \u200b Siswa itu sedang belajar bermain bola Artikal1 1 1 1 0 0 0 Artikel2 1 1 1 0 1 1 Untuk Codenya : => Source Code dibawah digunakan untuk menghitung setiap kata. def countWord(txt, ngram=1): d = dict() token = tokenisasi(txt, ngram) for i in token: if d.get(i) == None: d[i] = txt.count(i) return d => Source Code dibawah digunakan untuk membangun VSM. `def add_row_VSM(d): VSM.append([]) for i in VSM[0]: if d.get(i) == None: VSM[-1].append(0) else : VSM[-1].append(d.pop(i)); for i in d: VSM[0].append(i) for j in range(1, len(VSM)-1):","title":"VECTOR SPACE MODEL (VSM)"},{"location":"webcrawler/#vsmjinsert-20","text":"VSM[j].append(0) VSM[-1].append(d.get(i))` `cursor = conn.execute(\"SELECT * from jurnal2\") cursor = cursor.fetchall() cursor = cursor[:60] pertama = True corpus = list() label = list() c=1 n = int(input(\"ngram : \"))","title":"VSM[j].insert(-2,0)"},{"location":"webcrawler/#n1","text":"for row in cursor: print ('Proses : %.2f' %((c/len(cursor))*100) + '%'); c+=1 label.append(row[-1]) txt = row[-2] cleaned = preprosesing(txt) cleaned = cleaned[:-1] corpus.append(cleaned) d = countWord(cleaned, n) if pertama: pertama = False VSM = list((list(), list())) for key in d: VSM[0].append(key) VSM[1].append(d[key]) else: add_row_VSM(d)` => Source Code dibawah untuk menampilkan hasil CSV yang sudah kita create tadi. write_csv(\"bow_manual_%d.csv\"%n, VSM) feature_name = VSM[0] bow = np.array(VSM[1:]) ==========================","title":"n=1"},{"location":"webcrawler/#tf-idf","text":"TF - TF atau term frequency adalah weighting scheme yang digunakan untuk menentukan relevansi dokumen dengan sebuah query (term). TF menentukan bobot relevansi sebuah dokumen dan term berdasarkan frekuensi kemunculan term pada dokumen terkait. Untuk menghitung TF terdapat beberapa jenis fungsi yang dapat digunakan DF - Jika TF adalah frekuensi kemunculan suatu term pada sebuah dokumen, DF merupakan jumlah dokumen dimana terdapat term yang bersangkutan. Konsep DF sendiri dilatarbelakangin oleh masalah pada TF, dimana semua term dianggap sama penting, sehingga term yang memiliki sedikit atau tidak memiliki discrimination power dapat mempengaruhi akurasi dalam menentukan relevansi antara term dan dokumen. Ide dari DF adalah dengan mengurangi bobot TF suatu term dengan membaginya dengan frekuensi term terhadap koleksi dokumen (DF). Jadi sebuah term yang memiliki bobot TF yang besar namun dengan bobot DF yang besar pula tidak akan memiliki pengaruh yang besar dalam menentukan sebuah relevansi . IDF - IDF adalah inverse dari DF, IDF akan melakukan proses scaling pada TF. Term yang memiliki DF yang rendah akan memiliki IDF yang tinggi. Dengan kata lain, sebuah term yang jarang ditemui pada koleksi dokumen atau bisa dikatakan sebagai term khusus akan memiliki nilai IDF yang tinggi. Untuk menghitung IDF pada sebuah term pada sebuah koleksi dokumen dapat menggunakan fungsi dibawah ini, TF - IDF -- Selain menggunakan Bag of Words, kita juga bisa menggunakan metode TF-IDF. Hal ini karena Bag of Word memiliki kelemahan tersendiri. TF-IDF sendiri merupakan kepanjangan dari Term Frequence (frekuensi Kata) dan Invers Document Frequence (invers frekuensi Dokumen). Rumus TF-IDF sendiri terbilang mudah karena hanya TFxIDF. Kita telah mencari TF sebelumnya (yaitu Bag of Words), karena konsep keduanya yang memang sama. Sekarang kita tinggal mencari nilai IDF. Untuk mendapatkan IDF, pertama kita perlu mencari DF (frekuensi Dokumen). Contoh : Artikel 1 : siswa itu sedang belajar Artikel 2 : siswa itu sedang bermain bola Kata Jumlah Kata siswa 2 itu 2 sedang 2 belajar 1 bermain 1 bola 1 => Source Code df = list() total_doc = bow.shape[0] for kolom in range(len(bow[0])): total = 0 for baris in range(len(bow)): if (bow[baris, kolom] > 0): total +=1 df.append(total) df = np.array(df) idf = list() for i in df: tmp = 1 + log10(total_doc/(1+i)) idf.append(tmp) idf = np.array(idf) tfidf = bow * idf ==========================","title":"TF - IDF"},{"location":"webcrawler/#preprocessing","text":"Tahap preprocessing merupakan tahap mengolah data agar data lebih mudah diproses. Untuk teman-teman tidak diharuskan mengunakaan python saja, namun terserah teman-teman Tpi pada penejalasan ini saya menggunakan python Seleksi Fitur - Seleksi fitur merupakan salah satu cara untuk mengurangi dimensi fitur yang sangat banyak. Seperti pada kasus kita, Text Mining, jumlah fitur yang didapatkan bisa mencapai lebih dari 2000 kata yang berbeda. Namun, tidak semua kata tersebut benar-benar berpengaruh pada hasil akhir nantinya. Selain itu, kita tahu bahwa semakin banyak data yang diproses, maka lebih banyak biaya dan waktu yang digunakan untuk memprosesnya. Oleh karena itu, kita perlu melakukan pengurangan fitur tanpa mengurangi kualitas hasil akhir, misalnya dengan Seleksi Fitur. Pada dasarnya, seleksi fitur memiliki 3 tipe umum: Wrap Filter Embed => Source Code Seleksi Fitur def seleksiFiturPearson(data, threshold): global meanFitur meanFitur = meanF(data) u=0 while u < len(data[0]): dataBaru=data[:, :u+1] meanBaru=meanFitur[:u+1] v = u while v < len(data[0]): if u != v: value = pearsonCalculate(data, u,v) if value < threshold: dataBaru = np.hstack((dataBaru, data[:, v].reshape(data.shape[0],1))) meanBaru = np.hstack((meanBaru, meanFitur[v])) v+=1 data = dataBaru meanFitur=meanBaru if u%50 == 0 : print(u, data.shape) u+=1 return data ========================== Pearson Corrlation Pendekatan Pearson merupakan pendekatan paling sederhana. Pada pendekatan ini, setiap fitur akan dihitung korelasinya. Semakin tinggi nilainya, maka fitur tersebut semakin kuat korelasinya. Lalu fitur yang memiliki korelasi tinggi akan dibuang salah satunya. Pendekatan ini digunakan untuk data tipe numerik. Kodenya bisa dilihat dibawah ini def pearsonCalculate(data, u,v): \"i, j is an index\" atas=0; bawah_kiri=0; bawah_kanan = 0 for k in range(len(data)): atas += (data[k,u] - meanFitur[u]) * (data[k,v] - meanFitur[v]) bawah_kiri += (data[k,u] - meanFitur[u])**2 bawah_kanan += (data[k,v] - meanFitur[v])**2 bawah_kiri = bawah_kiri ** 0.5 bawah_kanan = bawah_kanan ** 0.5 return atas/(bawah_kiri * bawah_kanan) def meanF(data): meanFitur=[] for i in range(len(data[0])): meanFitur.append(sum(data[:,i])/len(data)) return np.array(meanFitur) ========================== Clustering Clustering merupakan pengelompokan data menjadi k-kelompok (dengan k merupakan banyak kelompok). Pengelompkan tersebut berdasarkan ciri yang mirip. Pada kasus ini, maka ciri yang mirip bisa diketahui dari kata yang menjadi ciri dari setiap dokumen. Metode Clustering sendiri ada banyak. Salah duanya adalah K-Means Clustering dan Fuzzy C-Means Clustering. Setelah dilakukan proses Clustering, perlu kita cari nilai Silhouette Coefficient untuk melihat apakah hasil cluster tersebut sudah bagus atau tidak. ========================== K-Means => Source Code dibawah ini digunakan untuk melakukan clustering. kmeans = KMeans(n_clusters=5, random_state=0).fit(tfidf_matrix.todense()) for i in range(len(kmeans.labels_)): print(\"Doc %d =>> cluster %d\" %(i+1, kmeans.labels_[i])) ========================== Klasifikasi Berbeda dengan Clustering yang mengelompokkan data, klasifikasi lebih mirip pada peramalan data dengan merujuk kategori. Contoh simplenya, seperti peramalan golf. Jika cuaca cerah, angin tidak kencang, maka akan diadakan pertandingan golf. Jika cuaca cerah tapi angin kencang, maka pertandingan golf dibatalkan. Proses ini biasanya memerlukan data latih dan data test. Untuk menguji keberhasilannya, diperlukan untuk menghitung akurasi atau confution matrix. Metode-metode yang sering dipakai untuk klasifikasi di antaranya Naive Bayes. ========================== Naive Bayes Naive bayes termasuk pada supervised learning berdasarkan teorema Bayes, dengan asumsi \"naive\", bahwa setiap fitur tidak saling terkait (independen). Terdapat beberapa jenis Naive Bayes, seperti Gaussian NB, Multinomial NB, dll. Namun pada kasus text classification, Multinomial lebih cocok digunakan. Sebelum itu, kita perlu membagi data menjadi dua bagian: data training, dan data test x_train, x_test, y_train, y_test = train_test_split(xBaru1, label, test_size=0.33, random_state=0) model = MultinomialNB().fit(x_train, y_train) predicted = model.predict(x_test) ==========================","title":"PREPROCESSING"},{"location":"webcrawler/#referensi","text":"https://github.com/sastrawi/sastrawi/wiki/Stemming-Bahasa-Indonesia https://www.codepolitan.com/stemming-word-dalam-carik-bot-59a9ef6e96088 https://www.suara.com/bola/bola-category/bola-indonesia http://id.dbpedia.org/page/Tokenisasi https://github.com https://python.org https://jupyter.org","title":"REFERENSI"}]}